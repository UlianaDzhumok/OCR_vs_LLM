{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2025-03-21T07:32:23.358356Z",
          "iopub.status.busy": "2025-03-21T07:32:23.358118Z",
          "iopub.status.idle": "2025-03-21T07:32:30.373092Z",
          "shell.execute_reply": "2025-03-21T07:32:30.371993Z",
          "shell.execute_reply.started": "2025-03-21T07:32:23.358325Z"
        },
        "id": "w_r1dUFo5zxn",
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install openai google-genai anthropic requests pillow jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-21T07:32:30.37494Z",
          "iopub.status.busy": "2025-03-21T07:32:30.374577Z",
          "iopub.status.idle": "2025-03-21T07:32:45.740502Z",
          "shell.execute_reply": "2025-03-21T07:32:45.739495Z",
          "shell.execute_reply.started": "2025-03-21T07:32:30.374904Z"
        },
        "id": "KJitzQHZ5zx9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%pip install -U -q \"google-generativeai>=0.8.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-21T07:32:45.742342Z",
          "iopub.status.busy": "2025-03-21T07:32:45.742067Z",
          "iopub.status.idle": "2025-03-21T07:32:49.96424Z",
          "shell.execute_reply": "2025-03-21T07:32:49.963247Z",
          "shell.execute_reply.started": "2025-03-21T07:32:45.742318Z"
        },
        "id": "7gX0a8va5zx-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2025-03-21T07:32:49.966137Z",
          "iopub.status.busy": "2025-03-21T07:32:49.965809Z",
          "iopub.status.idle": "2025-03-21T07:33:35.857258Z",
          "shell.execute_reply": "2025-03-21T07:33:35.856203Z",
          "shell.execute_reply.started": "2025-03-21T07:32:49.96611Z"
        },
        "id": "fmXO9dXI5zx-",
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes transformers huggingface_hub\n",
        "!pip install git+https://github.com/deepseek-ai/Janus.git\n",
        "!pip install git+https://github.com/Dao-AILab/flash-attention.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-21T07:33:35.858523Z",
          "iopub.status.busy": "2025-03-21T07:33:35.858296Z",
          "iopub.status.idle": "2025-03-21T07:33:35.862447Z",
          "shell.execute_reply": "2025-03-21T07:33:35.861605Z",
          "shell.execute_reply.started": "2025-03-21T07:33:35.858505Z"
        },
        "id": "0ldh66QI5zyA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"[your_json]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-21T07:33:35.863499Z",
          "iopub.status.busy": "2025-03-21T07:33:35.863213Z",
          "iopub.status.idle": "2025-03-21T07:36:09.513959Z",
          "shell.execute_reply": "2025-03-21T07:36:09.513289Z",
          "shell.execute_reply.started": "2025-03-21T07:33:35.863465Z"
        },
        "id": "wZFKRLas5zyA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from janus.utils.io import load_pil_images\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
        "\n",
        "cuda_device = 'cuda:0'\n",
        "\n",
        "# Specify the path to the model\n",
        "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
        "tokenizer = vl_chat_processor.tokenizer\n",
        "\n",
        "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path, trust_remote_code=True, quantization_config=quantization_config, torch_dtype=torch.bfloat16)\n",
        "\n",
        "vl_gp=vl_gpt.to(cuda_device)\n",
        "\n",
        "def multimodal_understanding(images, question, seed, top_p, temperature):\n",
        "    # Clear CUDA cache before generating\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Set seed\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"<|User|>\",\n",
        "            \"content\": f\"<image_placeholder>\\n{question}\",\n",
        "            \"images\": images,\n",
        "        },\n",
        "        {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
        "    ]\n",
        "\n",
        "    # Ensure images are properly formatted as PIL images\n",
        "    pil_images = [Image.fromarray(img) if isinstance(img, np.ndarray) else img for img in images]\n",
        "\n",
        "    prepare_inputs = vl_chat_processor(\n",
        "        conversations=conversation, images=pil_images, force_batchify=True\n",
        "    ).to(cuda_device, dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16)\n",
        "\n",
        "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
        "\n",
        "    outputs = vl_gpt.language_model.generate(\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        attention_mask=prepare_inputs.attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False if temperature == 0 else True,\n",
        "        use_cache=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-21T07:39:52.768804Z",
          "iopub.status.busy": "2025-03-21T07:39:52.768498Z",
          "iopub.status.idle": "2025-03-21T07:50:35.197183Z",
          "shell.execute_reply": "2025-03-21T07:50:35.19623Z",
          "shell.execute_reply.started": "2025-03-21T07:39:52.768782Z"
        },
        "id": "76NFZ0nM5zyB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import openai\n",
        "import anthropic\n",
        "#import google.generativeai as genai\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from PIL import Image\n",
        "import base64\n",
        "import io\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "import jiwer  # Import JiWER for WER computation\n",
        "\n",
        "user_secrets = UserSecretsClient()\n",
        "\n",
        "# API Keys (Replace with your actual keys)\n",
        "ANTHROPIC_API_KEY = user_secrets.get_secret(\"ANTROPIC_API_KEY\")\n",
        "OPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_API_KEY\")\n",
        "GOOGLE_API_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = Path(\"/kaggle/input/donkeysmallocr-cyrillic-printed-8\") # I used 50 images from https://huggingface.co/datasets/DonkeySmall/OCR-Cyrillic-Printed-8\n",
        "test_csv = dataset_path / \"test/test.txt\"\n",
        "\n",
        "df_test = pd.read_csv(test_csv, sep=\",\", names=[\"filename\", \"text\"])\n",
        "df_test[\"filepath\"] = df_test[\"filename\"].apply(lambda x: str(dataset_path / \"test\" / x))\n",
        "\n",
        "# Take 2 images for fast testing\n",
        "df_sample = df_test.sample(50)\n",
        "\n",
        "# Convert image to Base64\n",
        "def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "# OpenAI API requests\n",
        "def ocr_openai(image_path):\n",
        "    \"\"\" Perform OCR using OpenAI GPT-4o Vision API \"\"\"\n",
        "    try:\n",
        "        encoded_image = encode_image(image_path)\n",
        "        headers = {\"Authorization\": f\"Bearer {OPENAI_API_KEY}\", \"Content-Type\": \"application/json\"}\n",
        "\n",
        "        data = {\n",
        "            \"model\": \"gpt-4o\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are an AI assistant helping with OCR.\"},\n",
        "                {\"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": \"Extract russian text from this image. Return ONLY extracted text.\"},\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded_image}\"}}\n",
        "                ]}\n",
        "            ],\n",
        "            \"max_tokens\": 500\n",
        "        }\n",
        "\n",
        "        response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=data)\n",
        "\n",
        "        if response.status_code == 429:\n",
        "            print(\"OpenAI API quota exceeded. Skipping OpenAI OCR.\")\n",
        "            return \"No result (OpenAI Quota Exceeded)\"\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No result\")\n",
        "        else:\n",
        "            print(f\"OpenAI API Error {response.status_code}: {response.text}\")\n",
        "            return \"No result\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"OpenAI API Error: {e}\")\n",
        "        return \"No result\"\n",
        "\n",
        "# Anthropic API requests\n",
        "def ocr_anthropic(image_path):\n",
        "    \"\"\"Perform OCR using Anthropic Claude 3.7 Sonnet API.\"\"\"\n",
        "    try:\n",
        "        encoded_image = encode_image(image_path)\n",
        "\n",
        "        client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "\n",
        "        message = client.messages.create(\n",
        "            model=\"claude-3-7-sonnet-20250219\",\n",
        "            max_tokens=1024,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"image\",\n",
        "                            \"source\": {\n",
        "                                \"type\": \"base64\",\n",
        "                                \"media_type\": \"image/jpeg\",\n",
        "                                \"data\": encoded_image,\n",
        "                            },\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": \"Extract russian text from this image. Return ONLY extracted text.\",\n",
        "                        },\n",
        "                    ],\n",
        "                }\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        return message.content[0].text if message and message.content else \"No result\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Anthropic API Error: {e}\")\n",
        "        return \"No result\"\n",
        "\n",
        "\n",
        "def ocr_google(image_path):\n",
        "    \"\"\"Perform OCR using Google Gemini 2.0 Flash API.\"\"\"\n",
        "    try:\n",
        "         # Open image and convert to bytes\n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "            image_bytes = image_file.read()\n",
        "\n",
        "        # Convert to PIL Image (Gemini requires a valid Image object)\n",
        "        img = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "        client = genai.Client(location=\"us-central1\", project=\"serious-water-454012-a9\", vertexai=True)\n",
        "        #client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "        response = client.models.generate_content(\n",
        "            model=\"gemini-2.0-flash\",\n",
        "            contents=[\"Extract russian text from this image. Return ONLY extracted text.\", img],\n",
        "        )\n",
        "        if response and response.candidates:\n",
        "            return response.candidates[0].content.parts[0].text\n",
        "        else:\n",
        "            return \"No result\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Google Gemini API Error: {e}\")\n",
        "        return \"No result\"\n",
        "\n",
        "# Local DeepSeek usage\n",
        "def ocr_deepseek(image_path):\n",
        "    \"\"\" Perform OCR using Deepseek Janus Pro \"\"\"\n",
        "    try:\n",
        "        # Load the image\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        # Define inputs\n",
        "        question = \"Extract russian text from this image. Return ONLY extracted text.\"\n",
        "\n",
        "        seed = 42\n",
        "        top_p = 0.8\n",
        "        temperature = 0.5\n",
        "\n",
        "        # Call the function with the correct image format\n",
        "        pred_text=multimodal_understanding([image], question, seed, top_p, temperature)\n",
        "\n",
        "        if pred_text:\n",
        "            return pred_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Deepseek Janus Error: {e}\")\n",
        "        return \"No result\"\n",
        "\n",
        "# Process Batch Requests & Compute WER using JiWER\n",
        "cer_scores = []\n",
        "models = {\"GPT-4o\": ocr_openai, \"Claude 3.7\": ocr_anthropic, \"Gemini 2.0 Flash\": ocr_google, \"Deepseek Janus Pro\": ocr_deepseek}\n",
        "\n",
        "for _, row in df_sample.iterrows():\n",
        "    img_path = row[\"filepath\"]\n",
        "    true_text = row[\"text\"]\n",
        "\n",
        "    for model_name, ocr_function in models.items():\n",
        "        pred_text = ocr_function(img_path)  # Perform OCR\n",
        "\n",
        "        # Compute CER using JiWER\n",
        "        cer_score = jiwer.cer(true_text.strip().lower(), pred_text.strip().lower())\n",
        "\n",
        "        cer_scores.append({\n",
        "            \"Model\": model_name,\n",
        "            \"Image\": img_path,\n",
        "            \"CER\": cer_score,\n",
        "            \"Reference\": true_text,\n",
        "            \"Prediction\": pred_text\n",
        "        })\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_cer = pd.DataFrame(cer_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "560u3gUP5zyD"
      },
      "source": [
        "###### 6 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-21T07:50:35.302991Z",
          "iopub.status.busy": "2025-03-21T07:50:35.302755Z",
          "iopub.status.idle": "2025-03-21T07:50:35.314631Z",
          "shell.execute_reply": "2025-03-21T07:50:35.313818Z",
          "shell.execute_reply.started": "2025-03-21T07:50:35.302955Z"
        },
        "id": "afTSkUhk5zyE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Save to CSV\n",
        "df_cer.to_csv(\"/kaggle/working/cer_results.csv\", mode=\"w\", index=False)\n",
        "\n",
        "# Print confirmation\n",
        "print(\"CER results saved: /kaggle/working/cer_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-21T07:50:35.315813Z",
          "iopub.status.busy": "2025-03-21T07:50:35.315614Z",
          "iopub.status.idle": "2025-03-21T07:50:35.997808Z",
          "shell.execute_reply": "2025-03-21T07:50:35.996893Z",
          "shell.execute_reply.started": "2025-03-21T07:50:35.315795Z"
        },
        "id": "RhTLiqqf5zyF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Function to Plot Best & Worst Examples\n",
        "def plot_best_worst_examples(df_cer):\n",
        "    models = df_cer[\"Model\"].unique()\n",
        "\n",
        "    for model_name in models:\n",
        "        # Filter data for the current model\n",
        "        model_data = df_cer[df_cer[\"Model\"] == model_name]\n",
        "\n",
        "        if model_data.empty:\n",
        "            print(f\"No data available for model: {model_name}\")\n",
        "            continue  # Skip if no data is available\n",
        "\n",
        "        # Find best (min WER) and worst (max WER) examples\n",
        "        best_example = model_data.loc[model_data[\"CER\"].dropna().idxmin()]\n",
        "        worst_example = model_data.loc[model_data[\"CER\"].dropna().idxmax()]\n",
        "\n",
        "        # Function to load and plot image\n",
        "        def plot_example(example, title):\n",
        "            img_path = example[\"Image\"]\n",
        "            if not os.path.exists(img_path):\n",
        "                print(f\"Image not found: {img_path}\")\n",
        "                return  # Skip missing image\n",
        "\n",
        "            img = Image.open(img_path).convert(\"L\")\n",
        "            plt.imshow(img, cmap=\"gray\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"{title}\\nCER: {example['CER']:.2%}\\nPrediction: {example['Prediction'][:100]}\")\n",
        "\n",
        "        # Display images with best and worst WER\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "        plt.suptitle(f\"{model_name} - Best & Worst OCR Results\")\n",
        "\n",
        "        # Best Example\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plot_example(best_example, f\"Best (Min CER)\")\n",
        "\n",
        "        # Worst Example\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plot_example(worst_example, f\"Worst (Max CER)\")\n",
        "\n",
        "        plt.show()  # Show both images together\n",
        "\n",
        "# Show best & worst results for all models\n",
        "plot_best_worst_examples(df_cer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-21T07:50:35.998851Z",
          "iopub.status.busy": "2025-03-21T07:50:35.998638Z",
          "iopub.status.idle": "2025-03-21T07:50:36.290463Z",
          "shell.execute_reply": "2025-03-21T07:50:36.289792Z",
          "shell.execute_reply.started": "2025-03-21T07:50:35.998833Z"
        },
        "id": "oCcoCZah5zyF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot CER Distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.boxplot([df_cer[df_cer[\"Model\"] == model][\"CER\"] for model in df_cer[\"Model\"].unique()], labels=df_cer[\"Model\"].unique())\n",
        "plt.title(\"CER Distribution Across Models\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Character Error Rate (CER)\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Compute Average CER per Model\n",
        "avg_cer_per_model = df_cer.groupby(\"Model\")[\"CER\"].mean().reset_index()\n",
        "avg_cer_per_model = avg_cer_per_model.sort_values(by=\"CER\")  # Sort for readability\n",
        "\n",
        "# Show CER summary table\n",
        "fig, ax = plt.subplots(figsize=(10, 3))\n",
        "ax.axis(\"tight\")\n",
        "ax.axis(\"off\")\n",
        "table = ax.table(cellText=avg_cer_per_model.values, colLabels=avg_cer_per_model.columns, cellLoc=\"center\", loc=\"center\")\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "plt.title(\"CER Results Per Model\")\n",
        "plt.show()\n",
        "\n",
        "# Save results as CSV files\n",
        "df_cer.to_csv(\"/kaggle/working/cer_per_sample.csv\",mode=\"w\", index=False)\n",
        "avg_cer_per_model.to_csv(\"/kaggle/working/average_cer_per_model.csv\",mode=\"w\", index=False)\n",
        "\n",
        "# Print confirmation\n",
        "print(\"Results saved as CSV files:\")\n",
        "print(\"- CER per sample: /kaggle/working/cer_per_sample.csv\")\n",
        "print(\"- Average CER per model: /kaggle/working/average_cer_per_model.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6841937,
          "sourceId": 11007713,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6892858,
          "sourceId": 11062394,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
